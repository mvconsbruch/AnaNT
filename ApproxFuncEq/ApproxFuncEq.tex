\documentclass[a4paper,11pt]{article}
\pagenumbering{arabic}
\usepackage{../environment}

\author{Max von Consbruch}

\begin{document}

\begin{center}
    \huge{Approximate functional equation, what is it all about?!}
\end{center}

\section{A classical approximate functional equation} % (fold)
\label{sec:A classical approximate functional equation}

The aim of the approximate functional equation is to understand $\zeta$ (or more 
generally, any $L$-function) better in the critical strip $0 < \Re s < 1$. 
In this first part, we will focus on the case of $\zeta$. 
When we proved that $\zeta$ has a meromorphic continuation to $\Re s > 0$, we 
used partial summation on the dirichlet series, showing that for $\Re s > 1$, we have
\[
    \zeta(s) = \sum_{n=1}^\infty n^{-s} = \frac{s}{s-1} - s \int_1^\infty \{t\} t^{-s-1} \dc t,
\]
where the RHS is defined also for $\Re s > 0$. 

Given some $N > 0$, a similar expression arises if we use partial
summation on the truncated Dirichlet series, as
\[
    \sum_{n \leq N} n^{-s} = N^{1-s} + s \int_1^N \lfloor t \rfloor t^{-s-1} \dc t
    = \frac {N^{1-s}} {1-s} + \frac s{s-1} - s \int_1^N \{t\} t^{-s-1} \dc t.
\]
Now one might be tempted to comare the RHSs of the previous two equations. Writing 
$s = \sigma + \ic t$, we find
\[
    \zeta(s) - \sum_{n \leq N}n^{-s} = \frac{N^{1-s}}{s-1} - s \int_1^N \{t\}
    t^{-s-1} \dc t = \frac {N^{1-s}}{s-1} + O\left(\frac{\abs{s}}\sigma N^{-\sigma}
        \right).
\]
The important observation is that nothing goes wrong if we pass from $\Re s > 1$
to $\Re s > 0$! We found that $\zeta$ is approximated by the first terms 
in its dirichlet series, even in the critical strip. As is turns out, this 
approximation is not great, as we still have that annoying $\abs s$ in the $O$-term,
which forces us to choose $N$ large (roughly like $t^{1/\sigma}$) to make use
of this approximation. The crucial thing we missed in our approximation is that
$n^{it} = \ec^{(\log n) \ic t}$ oscillates and constitutes a lot of
cancellation. Using some sort of approximate fourier transform called 
\textit{van der Corput summation}, one can get hold of this oscillation to
obtain a stronger bound on the error, given by
\[
    \zeta(s) = \sum_{n \leq x} n^{-s} - \frac{x^{1-s}}{1-s} + O(x^{-\sigma}).
\]
This is uniform in $\sigma > \sigma_0$ once we fix $\sigma_0 > 0$, provided that
$\abs t \leq 4x$. (Take a look in Chapter 4 of Brüdern's book for details).

Choosing $\sigma = \frac 12$, we find that 
\[
    \zeta(s) \ll \sum_{n \leq t} n^{-\sigma} + t^{1-\sigma} \ll t^{1- \sigma},
\]
which is an okay bound, but not as good as we'd like. The convexity bound already 
gave that $\zeta(s) \ll t^{\frac{1-\sigma}2 + \varepsilon}$, so we could hope 
that we could do even better, approximating $\zeta$ with sums of length $\sqrt t$.
Unfortunately, it does not seem as if such a identity holds true. 

However, we can apply the functional equation to obtain a similar approximation
of $\zeta$, just from the other side (i.e., at $1-s$). Even better, we might be
able to combine these approximations to obtain a better approximation of 
$\zeta$. This is the Idea of the \textit{approximate functional equation}. 
And indeed, it gives what we hoped for: We can essentially approximate $\zeta$ by
Dirichlet-sums of length $\sqrt t$. If we write $\zeta(s) = \Delta(s) \zeta(1-s)$,
the theorem reads as
\begin{thm}[Approximate functional equation]
    Let $0 < \sigma < 1$ and $2 \pi xy = t$ , where $x,y > 1$. Then
    \[
    \zeta(s) = \sum_{n \leq x}n^{-s} + \Delta(s) \sum_{n \leq y} n^{s-1} 
    + O((x^{-\sigma} + t^{1/2-\sigma} y^{\sigma-1}) \log t).
    \]
\end{thm}
We shouldn't worry about the shape of the error term too much, just observe that 
the balanced case is given when $\sigma = \frac 12$ and $x=y= \sqrt{\frac x
{2\pi}}.$ 

This AFE is stronger than the one we had in the lecture. For example, it allows
us to deduce asymptotic formulas for the second and fourth moments of $\zeta$
on the critical line, only using elementary manipulations. We get
\[
    \int_0^T \abs{\zeta(\tfrac 12 + \ic t)}^2 \dc t = T \log T + O(T)
\]
and 
\[
    \int_0^T \abs{\zeta(\tfrac 12 + \ic t)}^4 \dc t = \frac{T (\log T)^4}{2\pi} + 
    O(T(\log T)^3).
\]
(Again, you can read this up in Brüdern's book, chapter 4).


% section A classical approximate functional equation

\section{The smoothed approximate functional equation} % (fold)
\label{sec:Our approximate functional equation}
Okay, the approximate functional equation is cool (or not), but how does it relate
to the approximate functional equation we had in the lecture?! It looked much
more complicated and did not
allow us to deduce asymptotic formulas. In the lecture, we proved a smoothed
version of the formula above, and usually, smoothed formulas are easier to
prove, but harder to use. (One reason for this phenomenon is that 
the mellin tranform transforms smoothness into rapid
decay along vertical lines. This usually makes calculations easy.)

We are given some Dirichlet $L$-function $L(s) = \sum_{n=1}^\infty a_n n^{-s}$,
which we want to approximate by a sum of the form 
$\sum_{n=1}^\infty a_n n^{-s} \omega(n/X)$, where $X>0$ and $\omega = \omega_s$
is some smooth cut-off function. Denoting the Mellin transform of $\omega$ by
$\hat \omega$, we obtain (similarly to what we did for Perron's formula)
\begin{equation}
    \sum_{n=1}^\infty a_n n^{-s} \omega\left(\frac nX \right) = \pifrac \int_{(c)} 
    L(s+u) X^u \hat \omega(u)  \dc u.
\end{equation}
Now, as we always do, we hope to find something new when shifting the contour 
of the integral to the left, to $(-c)$, say. We now that we want $L(s)$ to appear,
so it seems advisable to choose $\omega$ such that $\hat \omega$ has a simple
pole with residue $1$ at zero. We also might pick up residues of $L$ at 
$u = -s$ and $u = 1-s$, we will denote these residues with $R$. We get
\[
    \sum_{n=1}^\infty a_n n^{-s} \omega(n/X) = 
    L(s) + \pifrac \int_{(-c)} L(s+u) \hat \omega(u) X^u \dc u + R.
\]
What now? Usually we would like to bound the integral, but showing that this integral
is small seems impossible. However, as we understand $L(s)$ way better at 
$\Re s = c$ than at $\Re s = -c$, we could try to apply the functional equation.
The functional equation for $L$ reads
\[
    \Lambda(s) = \eta \bar \Lambda(1-s), \quad \text{where} \quad \Lambda(s) = 
    N^{s/2} L_\infty(s) L(s). 
\]
Hence we write $\hat \omega(u) = \frac{L_\infty(s+u) G(u)}{s L_\infty(s)}$, and we
impose that $G$ is a function independent of $u$ with $G(0) =1$. We obtain 
\begin{equation}
\sum_{n=1}^\infty a_n n^{-s} \omega(n/X)  = 
L(s) + \frac{\eta}{N^{s/2} L_\infty(s)} \cdot \frac1 {2\pi \ic} 
\int_{(-c)} \Lambda(s+u) G(u) \left( \frac X{\sqrt N} \right)^u \frac {\dc u} u + R
\end{equation} 
Investigating the integral further, we find after applying the functional equation 
and a few manipulations
\[
\begin{aligned}
    \pifrac \int_{(-c)} \Lambda(s+u) G(u) \left( \frac X{\sqrt N} \right)^u
    \frac {\dc u} u 
    &=  \pifrac \int_{(-c)} \Lambda(1-s-u) G(u) \left( \frac X{\sqrt N} \right)^u \frac {\dc u} u \\
    &=  -\pifrac \int_{(c)} \bar \Lambda(1-s+u) G(-u) \left( \frac {\sqrt N}X \right)^u \frac {\dc u} u \\
    &=  -\pifrac \int_{(c)} \bar \Lambda(1-s+u) G(-u) \left( \frac {\sqrt N}X \right)^u \frac {\dc u} u. 
\end{aligned}
\]
If we further impose that $G$ is an even function (i.e. $G(u) = G(-u)$), this is
almost the integral we started with in (2.1)! We can reverse our initial procedure
and obtain
\begin{multline*}
    \pifrac \int_{(c)} \bar \Lambda(1-s+u) G(-u) \left( \frac {\sqrt N}X \right)^u \frac {\dc u} u \\ = \frac{N^{\frac{1-s}2}L_\infty(1-s)}{2 \pi \ic} \int_{(c)}
    \bar L(1-s+u) \hat \omega_{1-s}(u) \left( \frac NX \right)^u \dc u
    = N^{\frac{1-s}2} L_\infty(1-s) \sum_{n=1}^\infty \frac{\bar{a_n}}{n^{1-s}}
    \omega_{1-s}\left(\frac{nX}N\right).
\end{multline*}
Plugging this into (2.2), we get 
\begin{equation}
    L(s) = \sum_{n=1}^\infty a_n n^{-s} \omega_s\left(\frac nX\right) + \eta N^{1/2 - s} \frac{L_\infty(1-s)}{L_\infty(s)} \sum_{n=1}^\infty \frac{\bar{a_n}}{n^{1-s}}
    \omega_{1-s}\left(\frac{nX}N\right) - R,
\end{equation}
which is exactly the statement of the approximate functional equation of the lecture
once we replace $\omega$ by the inverse mellin transform of $\hat \omega$, 
and $X$ by $X\sqrt N$. 

\textbf{TL;DR:} The smoothed approximate functional equation we had in the lecture
is just another instance of a recurrent procedure: 
We start with some (possibly smoothly) weighted sum $\sum_{n \in \N} a_n
\omega(n/x)$, calculate the mellin transform $\hat \omega$ of $\omega$ and obtain
for $x > 0$ 
$$ \sum_{n \in \N} a_n \omega(n/x) =\pifrac \int_{(c)} F(s) \hat \omega(s) x^s
\dc s. $$
We now shift the contour to the left, pick up residues and make up methods to
deal with the remaining integral. In our case, we really care more about what's 
happening in the integrals than in the sum, so we first choose a function we want to
have as mellin transform and then choose $\omega$ in a way such that it mellin
transforms into that function. (This leads to the awkward definition of $V_s$).
With this choice of a smooth weight we are able to pick up $L(s)$ as 
a residue (which is what we wanted to approximate) and use the 
functional equation for $L$-functions to reinterpret the shifted contour as 
a similar smoothly weighted sum again. This leads to (2.3).



% section Our approximate functional equation

\end{document}
